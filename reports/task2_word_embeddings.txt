Итоговый отчет после экспериментов и доработок:

1) Эксперимент: обучение word2vec с первоначальными настройками.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 8, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 2

Итоги:
- среднее значение функции потерь на обучении 0.8782513796657636
- среднее значение функции потерь на валидации 0.8773798287580551

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.99993825),
#  ('duck', 0.6317737),
#  ('thighs', 0.6019647),
#  ('breasts', 0.6009717),
#  ('stock', 0.578912),
#  ('broth', 0.54807943),
#  ('quarts', 0.5427128),
#  ('breast', 0.53645736),
#  ('sodium', 0.52064306),
#  ('apart', 0.51123625)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('emmenthal', 1.058097),
#  ('cheeses', 1.0083457),
#  ('cottage', 0.9961007),
#  ('gorgonzola', 0.99484754),
#  ('glaze', 0.9869747),
#  ('cheese', 0.97797066),
#  ('sheep', 0.9763118),
#  ('cacao', 0.9730935),
#  ('monterey', 0.96461684),
#  ('gruyère', 0.9633371)]

###

2) Эксперимент: увеличил количество эпох для обучения до 100. 
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 8, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 100

Итоги:
- среднее значение функции потерь на обучении 0.8641595727559747
- среднее значение функции потерь на валидации 0.86721510328824

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.99993926),
#  ('turkey', 0.60417765),
#  ('stock', 0.5805927),
#  ('breast', 0.55212885),
#  ('broth', 0.5369444),
#  ('beef', 0.5276002),
#  ('skinless', 0.5217426),
#  ('boneless', 0.52140826),
#  ('legs', 0.4949334),
#  ('breasts', 0.49346253)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0839916),
#  ('cheese', 0.9028638),
#  ('goat', 0.7649341),
#  ('fromage', 0.73087496),
#  ('stilton', 0.66413754),
#  ('grating', 0.6562606),
#  ('shaving', 0.648841),
#  ('crema', 0.64870787),
#  ('cottage', 0.64738345),
#  ('provolone', 0.63000137)]

Выводы:
Обучение остановилось на 46 эпохе. Для обучения модели с первоначальными настройками достаточно 30-40 эпох.

###

3) Эксперимент: изменение параметров обучения.
Количество отрицательных слов увеличил с 25 до 50, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 50, BATCH_SIZE = 8, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.8668535738096185
- среднее значение функции потерь на валидации 0.8683050898855867

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999358),
#  ('turkey', 0.6009294),
#  ('beef', 0.5491006),
#  ('thighs', 0.53819746),
#  ('stock', 0.52533925),
#  ('breasts', 0.5001553),
#  ('breast', 0.49786338),
#  ('broth', 0.48782355),
#  ('boneless', 0.46828187),
#  ('duck', 0.46470287)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0832565),
#  ('cheese', 0.83762413),
#  ('sandwich', 0.6945142),
#  ('percent', 0.69018906),
#  ('cheddar', 0.68463),
#  ('solids', 0.6593648),
#  ('chocolate', 0.6523241),
#  ('pignoli', 0.6273376),
#  ('fontina', 0.6145799),
#  ('cool', 0.60679525)]

Выводы:
Значение функции потерь практически не изменилось.

###

4) Эксперимент: изменение параметров обучения.
Количество отрицательных слов уменьшил с 25 до 10, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 10, BATCH_SIZE = 8, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.8690798600753029
- среднее значение функции потерь на валидации 0.8707712382766087

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.99993265),
#  ('stock', 0.6177794),
#  ('turkey', 0.5775455),
#  ('broth', 0.57352984),
#  ('beef', 0.4823871),
#  ('skinless', 0.47605085),
#  ('duck', 0.47470194),
#  ('breast', 0.47451964),
#  ('boneless', 0.46757546),
#  ('lamb', 0.4670046)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0887142),
#  ('cheese', 1.0573212),
#  ('cheddar', 0.76874614),
#  ('ricotta', 0.72709167),
#  ('curaçao', 0.7254681),
#  ('kasha', 0.7100642),
#  ('cointreau', 0.70494723),
#  ('emmenthal', 0.70455694),
#  ('taleggio', 0.7030445),
#  ('fontina', 0.68165153)]

Выводы:
Значение функции потерь на валидации увеличилось.

###

5) Эксперимент: изменение параметров обучения.
Размер батча увеличил с 8 до 32, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.8629807177512184
- среднее значение функции потерь на валидации 0.8665045019485469

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999429),
#  ('duck', 0.6132592),
#  ('turkey', 0.5925722),
#  ('thighs', 0.5827576),
#  ('beef', 0.5808613),
#  ('stock', 0.57502574),
#  ('pheasant', 0.5446095),
#  ('breast', 0.5433969),
#  ('magret', 0.5415671),
#  ('breasts', 0.53393215)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0573564),
#  ('cheese', 0.86120933),
#  ('solids', 0.85443354),
#  ('percent', 0.76086473),
#  ('semisweet', 0.749452),
#  ('feta', 0.72021306),
#  ('mozzarella', 0.68888557),
#  ('goat', 0.6766526),
#  ('dairy', 0.6728854),
#  ('bittersweet', 0.6688864)]

Выводы:
Значение функции потерь на обучении и валидации немного уменьшилось.

###

6) Эксперимент: изменение параметров обучения.
Размер батча увеличил до 64, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 64, LEARNING_RATE = 1e-2, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.8616479046190193
- среднее значение функции потерь на валидации 0.8665471084770702

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999442),
#  ('turkey', 0.6621525),
#  ('beef', 0.58403903),
#  ('simmering', 0.573722),
#  ('veal', 0.5607168),
#  ('duck', 0.5597532),
#  ('breasts', 0.5518374),
#  ('defatted', 0.5514477),
#  ('stock', 0.5492122),
#  ('wings', 0.5294162)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 0.99293387),
#  ('cheese', 0.9420042),
#  ('comté', 0.7334508),
#  ('gruyère', 0.7112636),
#  ('percent', 0.6755778),
#  ('mozzarella', 0.6691625),
#  ('queso', 0.6625265),
#  ('reggiano', 0.65478945),
#  ('feta', 0.65020484),
#  ('goat', 0.6439855)]

Выводы:
Значение функции потерь на обучении и валидации еще немного уменьшилось.

###

7) Эксперимент: изменение параметров обучения.
Увеличил размер батча до 32 и уменьшил коэффициент скорости обучения до 1e-4, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 1e-4, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.8824011645336142
- среднее значение функции потерь на валидации 0.8815140676257013

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999144),
#  ('canned', 0.94165695),
#  ('homemade', 0.9412342),
#  ('stock', 0.9396274),
#  ('broth', 0.93675333),
#  ('sodium', 0.9364884),
#  ('cups', 0.9349794),
#  ('beef', 0.8978295),
#  ('cooked', 0.8788797),
#  ('fish', 0.87810194)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cheese', 0.99675965),
#  ('parmigiano', 0.9832124),
#  ('reggiano', 0.98291683),
#  ('parmesan', 0.98125255),
#  ('pecorino', 0.97260106),
#  ('crumbled', 0.9670811),
#  ('romano', 0.9624554),
#  ('feta', 0.9620858),
#  ('queso', 0.96200967),
#  ('cheddar', 0.9616387)]

Выводы:
Значение функции потерь на обучении и валидации увеличилось.

###

8) Эксперимент: изменение параметров обучения.
Увеличил размер батча до 32 и увеличил коэффициент скорости обучения до 1e-1, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 50, BATCH_SIZE = 32, LEARNING_RATE = 1e-1, RADIUS = 5, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 1.2975692402297767
- среднее значение функции потерь на валидации 1.2939854024988757

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9998363),
#  ('from', 0.56446975),
#  ('cups', 0.47282356),
#  ('water', 0.47023183),
#  ('juice', 0.47019985),
#  ('stock', 0.46491733),
#  ('cheese', 0.45918557),
#  ('reduced', 0.4260769),
#  ('pieces', 0.41239375),
#  ('trimmed', 0.40928113)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cheese', 1.1192539),
#  ('cacao', 0.9415681),
#  ('shavings', 0.7742074),
#  ('grated', 0.71279985),
#  ('drained', 0.5989969),
#  ('chopped', 0.58189106),
#  ('chicken', 0.5706286),
#  ('swiss', 0.5572931),
#  ('finely', 0.52816516),
#  ('crumbled', 0.52166677)]

Выводы:
Значение функции потерь на обучении и валидации значительно увеличилось.

###

9) Эксперимент: изменение параметров обучения.
Увеличил радиус с 5 до 10, размер батча взял равным 32, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 10, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 1.071437490099612
- среднее значение функции потерь на валидации 1.075365859742247

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.99994314),
#  ('beef', 0.61769545),
#  ('thighs', 0.6101752),
#  ('stock', 0.5945201),
#  ('turkey', 0.59416103),
#  ('duck', 0.58576083),
#  ('breast', 0.56617165),
#  ('broth', 0.55948466),
#  ('thigh', 0.55248797),
#  ('pheasant', 0.54196554)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 0.99674773),
#  ('cheese', 0.9041011),
#  ('solids', 0.78035146),
#  ('feta', 0.7472901),
#  ('percent', 0.7292347),
#  ('aged', 0.7071472),
#  ('quality', 0.6964627),
#  ('chocolate', 0.6910054),
#  ('semisweet', 0.6868513),
#  ('pecorino', 0.67927194)]

Выводы:
Значение функции потерь на обучении и валидации значительно увеличилось.

###

10) Эксперимент: изменение параметров обучения.
Уменьшил радиус с 5 до 3, размер батча взял равным 32, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.7549888356991377
- среднее значение функции потерь на валидации 0.7584996795356096

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999431),
#  ('duck', 0.623288),
#  ('squab', 0.59050083),
#  ('turkey', 0.58923876),
#  ('beef', 0.5777409),
#  ('pheasant', 0.5665296),
#  ('magret', 0.56560165),
#  ('veal', 0.5515817),
#  ('tenders', 0.5253355),
#  ('breast', 0.519425)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0133879),
#  ('cheese', 0.9089246),
#  ('cotija', 0.76182395),
#  ('solids', 0.7570162),
#  ('mozzarella', 0.73032445),
#  ('dairy', 0.7293955),
#  ('semisweet', 0.71574247),
#  ('bittersweet', 0.7141081),
#  ('feta', 0.6986851),
#  ('mascarpone', 0.68855715)]

Выводы:
Значение функции потерь на обучении и валидации значительно уменьшилось. Уменьшение размера окна с 5 до 3 улучшило качество модели.

###

11) Эксперимент: изменение параметров обучения.
Увеличил размер эмбеддинга с 100 до 200, радиус взял равным 3, размер батча взял равным 32, количество эпох взял равным 10.
Настройки: CUSTOM_TOKENIZER, VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 200, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.7543337875637396
- среднее значение функции потерь на валидации 0.7586503598291012

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999464),
#  ('turkey', 0.52912366),
#  ('duck', 0.5241505),
#  ('beef', 0.5076337),
#  ('stock', 0.4711008),
#  ('thighs', 0.47070706),
#  ('breasts', 0.45970473),
#  ('broth', 0.4564106),
#  ('breast', 0.4509083),
#  ('boneless', 0.4394171)]

#  embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.0138912),
#  ('cheese', 0.88537705),
#  ('percent', 0.6390385),
#  ('solids', 0.6188462),
#  ('mozzarella', 0.6089299),
#  ('feta', 0.6036745),
#  ('ricotta', 0.5694385),
#  ('pecorino', 0.56724435),
#  ('reggiano', 0.55978274),
#  ('muenster', 0.5438813)]

Выводы:
Значение функции потерь на обучении и валидации практически не изменилось.

###

12) Эксперимент: снятие ограничения на минимальную длину токенов для кастомного токенизатора.
(Без лемматизации и pos-тэггинга)
Настройки: CUSTOM_TOKENIZER (min_token_size=0), VOCAB_SIZE = 2267, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.675302317802576
- среднее значение функции потерь на валидации 0.6788869391529954

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999457),
#  ('turkey', 0.66110843),
#  ('beef', 0.62912214),
#  ('duck', 0.60853827),
#  ('rabbit', 0.556271),
#  ('stock', 0.5547218),
#  ('breasts', 0.53945774),
#  ('pheasant', 0.53582895),
#  ('thigh', 0.5347942),
#  ('gizzards', 0.51673585)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.1092119),
#  ('cheese', 0.8843621),
#  ('grana', 0.75664973),
#  ('feta', 0.72646654),
#  ('blanco', 0.6889404),
#  ('cheddar', 0.6693572),
#  ('percent', 0.66085273),
#  ('peeler', 0.64225477),
#  ('solids', 0.64150566),
#  ('padano', 0.6404688)]

Выводы:
Значение функции потерь довольно сильно уменьшилось.

###

13) Эксперимент: использование токенизатора spacy.
(Без лемматизации и POS-тэггинга)
Настройки: SPACY_TOKENIZER, VOCAB_SIZE = 2909, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.6634001735149175
- среднее значение функции потерь на валидации 0.6670184437897746

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999468),
#  ('duck', 0.6510455),
#  ('turkey', 0.6464188),
#  ('beef', 0.63391036),
#  ('stock', 0.5811796),
#  ('breasts', 0.5488124),
#  ('simmering', 0.5486686),
#  ('broth', 0.53084916),
#  ('thighs', 0.5237264),
#  ('rabbit', 0.521259)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 0.9802453),
#  ('cheese', 0.85011315),
#  ('salata', 0.79259133),
#  ('queso', 0.78089994),
#  ('log', 0.695465),
#  ('solids', 0.6878278),
#  ('sheep', 0.6810087),
#  ('ricotta', 0.67231363),
#  ('feta', 0.6662565),
#  ('percent', 0.652651)]

Выводы:
Значение функции потерь довольно низкое.

###

14) Эксперимент: использование токенизатора spacy.
Лемматизация.

Настройки: SPACY_TOKENIZER (Lemmatizing), VOCAB_SIZE = 2421, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.6652701859888823
- среднее значение функции потерь на валидации 0.6684435049006455

# embeddings.most_similar('chicken')
# Out:
# [('chicken', 0.9999483),
#  ('turkey', 0.6246453),
#  ('duck', 0.6230294),
#  ('stock', 0.58998185),
#  ('broth', 0.5781242),
#  ('veal', 0.5665823),
#  ('quail', 0.5465463),
#  ('beef', 0.5460892),
#  ('breast', 0.5383717),
#  ('duckling', 0.53052694)]

# embeddings.analogy('cake', 'cacao', 'cheese')
# Out:
# [('cacao', 1.1129448),
#  ('cheese', 0.82626855),
#  ('sheep', 0.78516567),
#  ('calvado', 0.68318707),
#  ('percent', 0.6778603),
#  ('Gruyere', 0.6663411),
#  ('Branca', 0.6558141),
#  ('Reserve', 0.6306745),
#  ('fromage', 0.6076672),
#  ('cassis', 0.60467666)]

Выводы:
Значение функции потерь довольно низкое.

###

15) Эксперимент: использование токенизатора spacy.
POS-тэггинг.

Настройки: SPACY_TOKENIZER (POS-tagging), VOCAB_SIZE = 3264, NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
Среднее значение функции потерь на обучении 0.6631953794321139
Среднее значение функции потерь на валидации 0.66771803032152

# embeddings.most_similar('chicken_NOUN')
# Out:
# [('chicken_NOUN', 0.99994254),
#  ('turkey_NOUN', 0.6950198),
#  ('pheasant_NOUN', 0.61642224),
#  ('duck_NOUN', 0.59897506),
#  ('beef_NOUN', 0.5794192),
#  ('veal_NOUN', 0.57674444),
#  ('stock_NOUN', 0.54830897),
#  ('breasts_NOUN', 0.5437427),
#  ('broth_NOUN', 0.5186801),
#  ('wing_NOUN', 0.51798356)]

# embeddings.analogy('cake_NOUN', 'cacao_NOUN', 'cheese_NOUN')
# Out:
# [('cacao_NOUN', 0.99485296),
#  ('cheese_NOUN', 0.9527608),
#  ('Emmenthal_PROPN', 0.8301099),
#  ('Comté_PROPN', 0.73161066),
#  ('citron_NOUN', 0.7031032),
#  ('Pecorino_ADJ', 0.6955084),
#  ('Fontina_PROPN', 0.6898443),
#  ('chocolate_NOUN', 0.6848223),
#  ('pepperoni_NOUN', 0.6662842),
#  ('sheep_NOUN', 0.6651516)]

Выводы:
Значение функции потерь довольно низкое.

###

16) Эксперимент: использование токенизатора spacy.
Лемматизация POS-тэггинг.

Настройки: SPACY_TOKENIZER (Lemmatizing, POS-tagging), VOCAB_SIZE = 2774 , NEGATIVE_SAMPLES_N = 25, BATCH_SIZE = 32, LEARNING_RATE = 0.01, RADIUS = 3, EMBEDDING_SIZE = 100, EPOCH_N = 10

Итоги:
- среднее значение функции потерь на обучении 0.6641513660870333
- среднее значение функции потерь на валидации 0.6678653430058319

# embeddings.most_similar('chicken_NOUN')
# Out:
# [('chicken_NOUN', 0.9999471),
#  ('stock_NOUN', 0.5937826),
#  ('turkey_NOUN', 0.56244886),
#  ('duck_NOUN', 0.5497401),
#  ('broth_NOUN', 0.53921604),
#  ('pheasant_NOUN', 0.5330454),
#  ('veal_NOUN', 0.5325875),
#  ('beef_NOUN', 0.50705683),
#  ('skate_NOUN', 0.5021599),
#  ('breast_NOUN', 0.5007763)]

# embeddings.analogy('cake_NOUN', 'cacao_NOUN', 'cheese_NOUN')
# Out:
# [('cacao_NOUN', 1.0826366),
#  ('cheese_NOUN', 0.96782404),
#  ('70_NUM', 0.73867005),
#  ('queso_NOUN', 0.71897376),
#  ('percent_NOUN', 0.70799565),
#  ('solid_NOUN', 0.6984317),
#  ('blanco_NOUN', 0.67209536),
#  ('Muenster_PROPN', 0.67171854),
#  ('Gruyère_PROPN', 0.6555697),
#  ('chocolate_NOUN', 0.6345702)]

Выводы:
Значение функции потерь довольно низкое.

###

17) Эксперимент: обучение fasttext из gensim.
Лемматизация, fasttext из gensim.

Настройки: SPACY_TOKENIZER (Lemmatizing), gensim.models.FastText

Итоги:
# fasttext.wv.most_similar('chicken_NOUNT')
# Out:
# [('chicken', 0.9287378787994385),
#  ('chick', 0.7206724882125854),
#  ('duck', 0.6634989976882935),
#  ('thigh', 0.6427212357521057),
#  ('turkey', 0.6352545022964478),
#  ('rabbit', 0.6288781762123108),
#  ('stock', 0.6000908017158508),
#  ('pheasant', 0.5917993783950806),
#  ('drumstick', 0.5890626907348633),
#  ('broth', 0.5815339684486389)]

###